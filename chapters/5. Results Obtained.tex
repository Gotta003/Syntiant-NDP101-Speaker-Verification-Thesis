\chapter{Results Obtained}
\label{cha:results obtained}
\section{Test Models}
\label{sec:test models}
This chapter shows the performances of the algorithms previously discussed, focusing on a standard version developed on Edge Impulse\cite{edgeimpulse_kws_example}, two d-vector versions, one emulating an existing solution\cite{dvector_extractor_TinySV} (d-vector extractor size 256) and another solution with size 128, that could be adapted to the system, to reduce the required dimension in the dataset storage, and five distilled models (one from the 128 version and four from the 256 version). The reason why various versions of the 256 d-vector size were generated is because of lack of fitting. To be deployed on Syntiant after distillation, the models have to be quantized in 4-int weights. Knowing that the model can host at most 589.000 4-int parameters, the solution for the 128 version is straightforward in converting to a consistent dense model; in contrast, the 256 version is harder to adapt to these tiny dimensions. To determine if a model can be hosted in a Syntiant NDP101, knowing that the dense layers are 3 intermediate and one output. The formula for computing the model size of a dense neural network is the following:
\begin{equation*}
    model\_size=\sum_{i=1}^{N}((in\_size_i+1)*out\_size_i)\,\,\,\,\,condition:\,\,model\_size<294.500\,bytes
\end{equation*} 
Instead, to compute the one of a convolutional neural network should be used this one because of multidimensionality, considering a general filter shape like (width, height, channels):
\begin{equation*}
    model\_size=\sum_{i=1}^{N}((in\_channels*f\_width*f\_height+1)*out\_channels)
\end{equation*}
In the following, there is \textbf{Table \ref{table:model sizes}} summarizing which model will be deployable on Syntiant:
\begin{table}[!h]
\centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Model Name & KWS & SV128 & SV256 & SVD128 & SVDU256 & SVD192 & SVD240 & SVD2256\\
        \hline
        Type & Dense & Conv & Conv & Dense & Dense & Dense & Dense & Dense\\
        Origin & - & - &  - & SV128 & SV256 & SV256 & SV256 & SV256\\
        Input & 1600 & (40x40x1) & (40x40x1) & 1600 & 1600 & 1600 & 1600 & 1600\\
        Layer 1 & 256 & (13x13x8) & (13x13x8) & 256 & 256 & 192 & 240 & 256\\
        Layer 2 & 256 & (6x6x16) & (6x6x16) & 256 & 256 & 192 & 240 & 256\\
        Layer 3 & 256 & (3x3x32) & (3x3x32) & 256 & 128 & 192 & 240 & 256\\
        Layer 4 & - & (2x2x64) & - & - & - & - & - & -\\
        Layer Out & 2 & 128 & 256 & 128 & 256 & 256 & 256 & 256\\
        Total (KB) & 2117 & 383,6 & 95,2 & 2243,5 & 2115,5 & 1683,25 & 2193,8 & 2372\\
        Deployable & YES & NO & NO & YES & YES & YES & YES & NO\\
        \hline
    \end{tabular}
    \caption{Model Dimension Details}
    \label{table:model sizes}
\end{table}
Note that the width and height of the convolution formula are used for the filter size; instead, in the table above, there are the input and the output of the steps. The filter is always (3x3xchannels), with channels instead of the ones present in the datasheet. The condition for a model to be deployable in float configuration, before quantization, is that it should be smaller than 2300 KB and it should be a dense model. The choice of the distillation models having as father 256-size version are, respectively:\newline
• U256 - consisting of an unbalanced distribution of neurons, having a layer downscaling to 218 from the standard of 256 to fit inside the Flash memory.\newline
• 192 - considering that model neurons should typically have a power of two, considering that 128 neurons in all layers is too restrictive to mimic the father's behavior, a general multiple of two with a reasonable value that fits inside the device was chosen.\newline
