\chapter{Results Obtained}
\label{cha:results obtained}
\section{Test Models}
\label{sec:test models}
This chapter shows the performances of the algorithms previously discussed, focusing on a standard version developed on Edge Impulse\cite{edgeimpulse_kws_example}, two d-vector versions, one emulating an existing solution\cite{dvector_extractor_TinySV} (d-vector extractor size 256) and another solution with size 128, that could be adapted to the system, to reduce the required dimension in the dataset storage, and five distilled models (one from the 128 version and four from the 256 version). The reason why various versions of the 256 d-vector size were generated is because of lack of fitting. To be deployed on Syntiant after distillation, the models have to be quantized in 4-int weights. Knowing that the model can host at most 589.000 4-int parameters, the solution for the 128 version is straightforward in converting to a consistent dense model; in contrast, the 256 version is harder to adapt to these tiny dimensions. To determine if a model can be hosted in a Syntiant NDP101, knowing that the dense layers are 3 intermediate and one output. The formula for computing the model size of a dense neural network is the following:
\begin{equation*}
    model\_size=\sum_{i=1}^{N}((in\_size_i+1)*out\_size_i)\,\,\,\,\,condition:\,\,model\_size<294.500\,bytes
\end{equation*} 
Instead, to compute the one of a convolutional neural network should be used this one because of multidimensionality, considering a general filter shape like (width, height, channels):
\begin{equation*}
    model\_size=\sum_{i=1}^{N}((in\_channels*f\_width*f\_height+1)*out\_channels)
\end{equation*}
In the following, there is \textbf{Table \ref{table:model dimension}} summarizing which model will be deployable on Syntiant:
\begin{table}[!h]
    \label{tabel:model sizes}
\centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Model Name & KWS & SV128 & SV256 & SVD128 & SVDU256 & SVD192 & SVD240 & SVD2256\\
        \hline
        Type & Dense & Conv & Conv & Dense & Dense & Dense & Dense & Dense\\
        Origin & - & - &  - & SV128 & SV256 & SV256 & SV256 & SV256\\
        Input & 1600 & (40x40x1) & (40x40x1) & 1600 & 1600 & 1600 & 1600 & 1600\\
        Layer 1 & 256 & (13x13x8) & (13x13x8) & 256 & 256 & 192 & 240 & 256\\
        Layer 2 & 256 & (6x6x16) & (6x6x16) & 256 & 256 & 192 & 240 & 256\\
        Layer 3 & 256 & (3x3x32) & (3x3x32) & 256 & 128 & 192 & 240 & 256\\
        Layer 4 & - & (2x2x64) & - & - & - & - & - & -\\
        Layer Out & 2 & 128 & 256 & 128 & 256 & 256 & 256 & 256\\
        Total (KB) & 2117 & 383,6 & 95,2 & 2243,5 & 2115,5 & 1683,25 & 2193,8 & 2372\\
        Deployable & YES & NO & NO & YES & YES & YES & YES & NO\\
        \hline
    \end{tabular}
    \label{table:model dimension}
    \caption{Model Dimension Details}
\end{table}
Note that the width and height of the convolution formula are used for the filter size; instead, in the table above, there are the input and the output of the steps. The filter is always (3x3xchannels), with channels instead of the ones present in the datasheet. The condition for a model to be deployable in float configuration, before quantization, is that it should be smaller than 2300 KB and it should be a dense model. The choice of the distillation models having as father 256-size version are, respectively:\newline
• U256 - consisting of an unbalanced distribution of neurons, having a layer downscaling to 218 from the standard of 256 to fit inside the Flash memory.\newline
• 192 - considering that model neurons should typically have a power of two, considering that 128 neurons in all layers is too restrictive to mimic the father's behavior, a general multiple of two with a reasonable value that fits inside the device was chosen.\newline
• 240 - the optimal multiple of 2 that could fit inside the device, having all the intermediate layers with equal neurons and the output of 256.\newline
• 256 - this model does not fit, but was used to compare the behavior with another model with 256 neurons in intermediate layers and 128 d-vector as output\newline
If the KWS performs a classification having a direct output, SV performs a cosine similarity with d-vectors saved as references. This is done to view various perspectives on which version of the model is better considering performance and size perspectives. The methodologies of d-vector comparison used are 3:\newline
1. Best matching - takes the highest cosine similarity value when comparing each reference vector with the vector obtained as a result of the Speaker Verification model. It requires that all reference vectors be saved in memory.\newline
2. Mean d-vector - the mean d-vector is computed performing the average among each d-vector and then using this obtained vector is performed a cosine similarity and the score obtained will be the result. In this case, only the mean d-vector should save more space.\newline
3. Geometrical Median - it is an optimized version of mean computation. Implements Weiszfeld's algorithm\cite{weiszfeldsalgorithm}, which is an iterative method to compute the geometric median of a set of points present in an Euclidean space. It is more computationally expensive because it first requires a mean computation. Then, some weights computation to minimize the distance between the points, to obtain an optimized d-vector. Weiszfeld's algorithm corresponds to \textbf{Algorithm \ref{algorithm:geometric media weiszfeld}} pseudocode.\newline
\begin{algorithm}[H]
\label{algorithm:geometric media weiszfeld}
\caption{Geometric Median via Weiszfeld’s Algorithm}
\KwIn{Set of $n$ vectors $\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\} \subset \mathbb{R}^d$}
\KwOut{Approximate geometric median $\mathbf{z} \in \mathbb{R}^d$}
\nl \textbf{Initialize:} Set initial estimate $\mathbf{z}^{(0)} \gets \frac{1}{n} \sum_{i=1}^n \mathbf{x}_i$\;
\nl \For{$t \gets 0$ \KwTo $T_{\max}$}{
    \nl Set $w_{\text{sum}} \gets 0$\;
    \nl Set $\mathbf{z}_{\text{new}} \gets \mathbf{0} \in \mathbb{R}^d$\;
    \nl \For{$i \gets 1$ \KwTo $n$}{
        \nl Compute $d_i \gets \|\mathbf{x}_i - \mathbf{z}^{(t)}\|$\;
        \nl \If{$d_i < \varepsilon$}{
            \nl \textbf{continue} \tcp*{Skip or optionally snap to $\mathbf{x}_i$}
        }
        \nl Compute weight $w_i \gets \frac{1}{d_i}$\;
        \nl Update weighted sum: $\mathbf{z}_{\text{new}} \gets \mathbf{z}_{\text{new}} + w_i \cdot \mathbf{x}_i$\;
        \nl Update weight sum: $w_{\text{sum}} \gets w_{\text{sum}} + w_i$\;
    }
    \nl \If{$w_{\text{sum}} = 0$}{
        \nl \textbf{break} \tcp*{All points are too close; cannot proceed}
    }
    \nl Normalize: $\mathbf{z}^{(t+1)} \gets \frac{\mathbf{z}_{\text{new}}}{w_{\text{sum}}}$\;
    \nl \If{$\|\mathbf{z}^{(t+1)} - \mathbf{z}^{(t)}\| < \varepsilon$}{
        \nl \textbf{break} \tcp*{Converged}
    }
}
\KwRet{$\mathbf{z}^{(t+1)}$}
\end{algorithm}
For each of these methods, the number of reference d-vectors used was 1, 8, 16 and 64; it is relevant to note that one result should be the same in all 3 modes. This reference dataset with size of 64 KB on a SRAM of 128 KB. The components of d-vector changes, because the default size-type model is float, but Syntiant quantizes it in 4-int weights, but the performances could not be verified because of the NDA, so were used in float format.\newline
Because of being a TinySV with an objective of using internal memory for dataset, too, it is important to consider the size, allowing evaluation on how many words and consequently how many users can be inserted in the logic depending on the usage. They are in inverse proportion with $num\,of\,users\cdot words\,categories=total\,num\,fitable\,on\,Syntiant$ and the total number in Syntiant varies according to the method used and the number of references. In the case of bestmatching technique, having a higher number of references leads to having less words, because a word representation will require more space. To map out the dataset dimension of each model, there is \textbf{Table \ref{table:word sizes}}\newline
\begin{table}[!h]
    \label{table:word sizes}
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{|c|cccccc|}
        \hline
        \textbf{Type} & \multicolumn{6}{c|}{\textbf{D-vector = 128}} \\
        \hline
        \textbf{Aggregation} & \multicolumn{4}{c}{\textbf{Best}} & \textbf{Mean} & \textbf{Geom\_Median} \\
        \hline
        \textbf{N° Refs} & \textbf{1} & \textbf{8} & \textbf{16} & \textbf{64} & \textbf{All} & \textbf{All} \\
        \hline 
        \textbf{Size of Word (B)} & 512 & 4096 & 8192 & 32768 & 512 & 512 \\
        \textbf{Quant (B)}        & 64 & 512 & 1024 & 4096 & 64 & 64 \\
        \textbf{Words Float}      & 128 & 16 & 8 & 2 & 128 & 128 \\
        \textbf{Words 4-int}      & 1024 & 128 & 64 & 16 & 1024 & 1024 \\
        \hline
        \textbf{Type} & \multicolumn{6}{c|}{\textbf{D-vector = 256}} \\
        \hline
        \textbf{Aggregation} & \multicolumn{4}{c}{\textbf{Best}} & \textbf{Mean} & \textbf{Geom\_Median} \\
        \hline
        \textbf{N° Refs} & \textbf{1} & \textbf{8} & \textbf{16} & \textbf{64} & \textbf{All} & \textbf{All} \\
        \hline 
        \textbf{Size of Word (B)} & 1024 & 8192 & 16384 & 65536 & 1024 & 1024 \\
        \textbf{Quant (B)}        & 128 & 1024 & 2048 & 8192 & 128 & 128 \\
        \textbf{Words Float}      & 64 & 8 & 4 & 1 & 64 & 64 \\
        \textbf{Words 4-int}      & 512 & 64 & 32 & 8 & 512 & 512 \\
        \hline
    \end{tabular}
    \caption{Dataset dimensional configuration based on aggregation type and number of references}
    \label{table:dataset_dimensions}
\end{table}
\section{Testing Dataset and Performance Metrics}
\label{sec:testing dataset and performance metrics}
The various models were tested with a composed dataset containing one-second audio samples of my own voice saying a train word (Sheila), the voice of other users saying the same word, but different from the ones used during training\cite{librispeech} and some samples of total different words and some that may sound like the chosen word. In the KWS model, the first two cases will output TP and FN, meanwhile the third category will output FP and TN. Instead, for SV models, only the words that were considered TP would be processed; however, considering that some reference d-vector had to be chosen randomly, an algorithm threshold was created. That one, in case the similarity was above 0.99, the sample has to be truncated. Because of this limitation in Convolutional Models, there are fewer samples, because they were eliminated to not ruin the results; instead, with distillation a loss in cosine similarity is predicted, because it is a try to mimic the convolution behavior with simple multiplication and addition, so even the reference samples passed in the neural network will not be processed as the same ones.\newline
Quantitatively speaking, the test dataset is made up of the following:\newline
• 485 Sheila Owner Samples (registered manual and split into one-second samples using Edge Impulse\cite{syntiant_tutorial_edgeimpulse})\newline
• 2022 Sheila Others Samples (taken by the Google Command Dataset\cite{speechcommands} corresponding to the Sheila word)\newline
• 474 Similar Sheila Words (taken by Google Command Dataset\cite{speechcommands} not corresponding to the Sheila word)\newline
This leads to a total of 2981 samples used for KWS; instead, for SV, to be sure that only true Sheila words were passed, were accepted only with a probability of being Sheila higher than 0.8, so the TP samples of KWS which are higher than that value.\newline
In addition to size metrics, there are some performance metrics previously introduced in \textbf{Chapter \ref{cha:background}} that will be used in evaluating the performance of each model. The threshold on which a value is determined to be true or not depends on the EER Threshold, which is the best value in which the FPR and the FNR are as close as possible. On this threshold, were computed:\newline
• Accuracy: Rate of results corresponding to expectations over total elements (Higher better)\newline
• Classification Loss: Rate of results not corresponding with expectations over total elements (Lower better)\newline
• Precision: Rate of true expectations over all samples which result is true (High better)\newline
• Recall: Rate of true results over all samples expected to be true (High better)\newline
• F1 score: An estimated score to evaluate performance considering precision and recall (High better)\newline
• EER: Is the highest value among the false acceptance rate and the false rejection rate, even though they should be similar, because the threshold is equal to the ERR point (Low better)\newline
• AUC: Is the area under the curve that in a binary classification that if around 0.5 indicates random guesses, instead higher values represent more deterministic results (High better)\newline
\section{Experiment performances}
\label{sec:experiment performance}
\subsection{KWS}
\label{subsec:kws performance}
Starting with the analysis of the results obtained, the first model to explore will be KWS. The classification is as known binary, but according to Syntiant, the maximum trainable keyword spotting words are 64.\cite{description_ndp101}. The computation of the performance calculation such as AUC and ERR was in a range of 0.5 and 1 thresholds. According to the results obtained, the threshold is around 0.7 with 90\% of true positive out of 2507, expected true samples, and 90,11\% of true negative out of 474 samples. The model overall performs well with a low Equal Error Rate and a decent AUC value, compensating with good accuracy, optimal precision, and good recall, leading to the following statistics:\newline
\begin{table}[!h]
\centering
%\caption{KWS Model Evaluation Results}
\small
\begin{tabular}{@{}m{3.8cm}@{\hspace{1em}}m{3.8cm}@{\hspace{1em}}m{3.8cm}@{\hspace{1em}}m{3.8cm}@{}}
\toprule
\multicolumn{1}{c}{\textbf{Configuration}} & \multicolumn{1}{c}{\textbf{Data Analysis}} & \multicolumn{1}{c}{\textbf{Performance}} & \multicolumn{1}{c}{\textbf{Quality}}\\
\midrule

\centering
\textbf{Model:} KWS \\
\textbf{Threshold:} 0.7 \\
\textbf{Total Samples:} 2981 &

\centering
\textbf{Confusion Matrix:} \\
TP: 2259, FN: 248 \\
FP: 48, TN: 426 \\[0.3em]
\textbf{Distribution:} \\
Pos: 2507 (84.1\%) \\
Neg: 474 (15.9\%) &

\centering
\textbf{Classification:} \\
Accuracy: 90.1\% \\
Precision: 97.9\% \\
Recall: 90.1\% \\
F1: 93.9\% &

\centering
\textbf{Quality:} \\
EER: 0.101 \\
AUC: 0.885 \\

\tabularnewline
\bottomrule
\end{tabular}
\end{table}
\subsection{Speaker Classification of CNN Model}
\label{subsec:speaker classification of cnn model}
\begin{figure}[!h]
    \begin{minipage}[b]{0.33\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/5.01 CCN Confusion Matrix.png}
            \caption{Confusion Matrix}
            \label{fig:confusion matrix}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.67\textwidth}
            \centering
            \includegraphics[width=0.8\textwidth]{images/5.02 CNN Training Details.png}
            \caption{CNN Training Details}
            \label{fig:cnn training details}
    \end{minipage}
\end{figure}
If KWS is only a single model that can be optimized and managed over time, the purpose of SV is to be a one-train model. Because of this, it is really important to determine the best configuration possible for a deployment. As a starting point, it is interesting to analyze the confusion matrix obtained by the classification of the original convolutional model before truncation. 
The training was performed with 700 epochs using as accuracy parameters the correspondence of the samples results with the expectations; instead, the loss was computed with two callbacks:\newline
1. ReduceLROnPlateau: In case the training loss does not improve (decrease), the learning rate is reduced by a factor. In this case, a patience of 10 and a factor of 0.5 were applied.\newline
2. EarlyStopping: In case the training loss does not improve, the training is stopped earlier. In this case, it was set to a patience of 20, but most importantly this allows one to restore the best weights from the epoch with the lowest training loss, to get the values, before the model overfits. The overfitting occurs when a model learns data too well, including outliers or random fluctuations, and performs poorly with unseen data.
As a result, an accuracy around 83\% is obtained; meanwhile, there is a 61\% loss. The reason why there is still a high loss is because of the presence of only an intermediate layer, other than the output one, dense layer's talking. This leads to less precision because mapping out 94 users may be difficult in a classification; however, all of this was done only to give the d-vector, to see how accurate it may be using a classification.
\subsection{SV Convolutional Models}
\label{subsec:sv convolutional models}
The two d-vector extractors obtained truncating the classification part are of 128-size and 256-size. The advantages of using the 128's one consists in size of the reference d-vectors, which in every case will be the half of 256's and could host the double of the words and it should be performing slightly better with the averaging methods, because it has fewer parameters. However, it will be less precise than 256's solution. These are what in theory is expected, but to verify the correctness in practise the two were viewed from two perspectives, one a general purpose usage, in which it is not important to have an absolute precision, but the objective is using the threshold which gives the maximum true positive rate and the minimum false positive rate, instead the other view is a security one, with authentication application, having a precision equal to 1. The complete results table are in \textbf{Table \ref{table:cnneer}} for the first case and in \textbf{Table \ref{table:cnnnofp}} for the other. In the comparison between the two models, an outperformance of 256 models over 128 one is seeable. Starting with the optimal EER, the analysis shows that the output size is a crucial factor to obtain better performance, increasing the representational capacity. Each method has the following characteristics:\newline
• BEST - show strong performance with an increasing number of references.\newline
• GEOM\_MEDIAN - it trails behind BEST, performing closely when the reference number is high, and it is a good compromise for robustness and simplicity, it is better with the 128 model, because of lower output element size.\newline
• MEAN - It shows trends similar to GEOM\_MEDIAN with 128 model and slightly better ones with 256.\newline
Across all methods, with an increasing reference number, it improves recall, precision and F1 score, and, at the same time, EER is reduced, leading to a better verification reliability. An interesting effect is that both models gain saturate beyond 16, with a slightly improved performance at 64. Considering the thesis objective, it is important that the model performs as best as possible, in the limit that it is user-friendly. In fact, registering 16 references to the system is more acceptable than 64. Below 16 the configurations struggle, showing poop performances. Even if the BEST approach shows better results than the others, it will occupy more memory, so there should be a balance between model accuracy and the size of a word saved on the MCU memory. The data of the models using 16 references are shown in \textbf{Figure \ref{fig:f1 score in configurations with 16 refs}}. 
Using these models it is recommendable in case there is space available to use BEST method with 256 configuration and use MEAN to have lower accuracy and precision, however, it is not recommendable to use GEOM\_MEDIAN, because it requires more computation than mean for lower performance, so in this case, probably because there were not many outliers, it has to be avoided. Considering the 16 reference configurations, other than GEOM\_MEDIAN performance in general in which 128 outperforms 256, it should be noted that 128 has in the MEAN method a better recall of about 6,41\% and an EER 3,61\% lower than the other. Instead, talking about no false positive tolerance analysis, because there is a try in maximizing the precision, parameters like Recall and EER, and as a consequence F1 Score, will be worse. It is seeable in \textbf{Table \ref{table:cnnnofp}}, that the 256-sized model significantly outperforms the 128-sized model, especially at higher reference numbers. It is interesting to note that unlike the EER configuration, the F1 score does not always increase, especially for the BEST method in 256-sized model, due to a drop in precision at higher NUM\_REFS. If BEST with 16 references is an optimal solution, because it maintains the values on track, even if it has to be accepted, it is a lower recall, but still acceptable. Instead, the other methods (GEOM\_MEDIAN and MEAN) are in any case not usable, because of recall lower than 50\%. In this case, to guarantee the model security it is required to sacrifice the size in favor of the optimization and there is no best method than 256-sized model with BEST method and 16 references. The results on 16 references of this no-tolerance false positive are shown in \textbf{Figure \ref{fig:f1 score in configurations with 16 refs (prec=1)}}.
\begin{figure}[!h]
    \begin{minipage}[b]{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/5.03 F1 Score 16 CNN.png}
            \caption{F1 Score with 16 References ($<$EER)}
            \label{fig:f1 score in configurations with 16 refs}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/5.04 F1 Score 16 CNN prec1.png}
            \caption{F1 Score with 16 References (PREC=1)}
            \label{fig:f1 score in configurations with 16 refs (prec=1)}
    \end{minipage}
\end{figure}
\subsection{Distillation Knowledge and SV Dense Neural Networks}
\label{sec:distillation knowledge and sv dense neural networks}
The models seen in the previous chapter were Convolutional; however, as introduced before, Syntiant NDP101 does not support this type of models, but only dense neural networks. Because of this reason, the distillation knowledge introduced in \textbf{Subsection \ref{subsec:knowledge distillation training}} should be applied to these two convolutional models. The test models with the new create dense configurations consist in the ones introduced in \textbf{Section \ref{sec:test models}}. It is important to remark that the model 256-256 does not fit inside Syntiant NDP101 and it is only taken as reference, because it should be an optimal case that should outperform the others proposed. This should happen because of no neuron's reduction and having the most trainable parameters that should mimic better than the others the behavior of the Convolutional Neural Network.
The training of this Dense Model is performed by parsing training input samples to the original model that will no modify its training parameters, then the student model tries to mimic the output of the master and the training parameters will modify according to the accuracy which will be cosine similarity and the loss 1-cosine similarity. The choice of this is due to achieve the objective to have the two models' output as close as possible. For possible future optimization of the models may be performed a layer per layer aware training, but to verify if the model can work with a standard distillation knowledge it was decided to focus on verify the possible versions deployable instead of optimizing them with more consistent solutions. In average, the cosine similarity of the output vectors is around 87,5\%. During the distillation of every models, the dataset of Speaker Verification was given in input to them. It contains only the samples that passed KWS, Sheila words above 80\% probability, so 2014 samples (453 positives and 1761 negatives). 
\begin{figure}[!h]
    \begin{minipage}[b]{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/5.05 F1 Score 16 DNN.png}
            \caption{F1 Score with 16 References ($<$EER)}
            \label{fig:f1 score in configurations with 16 refs dnn}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/5.06 F1 Score 16 DNN prec1.png}
            \caption{F1 Score with 16 References (PREC=1)}
            \label{fig:f1 score in configurations with 16 refs (prec=1) dnn}
    \end{minipage}
\end{figure}
\newpage
The results obtained by the five models in both minimum EER and precision equal to 1 case, can be seen respectively in \textbf{Table \ref{table:dnnerr}} and in \textbf{Table \ref{table:dnnnofp}}. Starting with minimum EER case, some patterns from Convolutional models are recurring, as fact, single reference give poor performances as are still unsufficient to be a solid structure. Going up with the number the accuracy and F1 Score continue increasing, arriving at 64 references that starts manifesting in some models so inefficiency in recall like saw in Convolutional results. As it is logical the EER performance decreases significantly with more references. According, to the reference perspective the best case should be the 16 references one, which results are displayed in \textbf{Table \ref{fig:f1 score in configurations with 16 refs dnn}}. From that it is seeable that BEST method achieves the best results as expected, GEOM\_MEDIAN gives more stable ones, minimizing the impact of some outliers and generally performs slightly better than MEAN method except in 256-256 case, which is not applicable in this case for Memory Flash space reasons, so overall to save space, even though it is a more energy consumption algorithm is recommended more GEOM\_MEDIAN. However, as said in \textbf{Subsection \ref{subsec:speaker classification of cnn model}}, using EER it is the threshold in which it is minimized maximizing TP and minimizing FP, but it does not guarantee security, so it is required a study on no FP tolerance version, which results are in \textbf{Figure \ref{fig:f1 score in configurations with 16 refs (prec=1) dnn}}.
In this case, happened that there where cases in which the TP were 0, so to avoid not useful data, the threshold was decreased, leading in same cases in having a precision not equal to 1. These cases can be recognized having a missing column in \textbf{Figure \ref{fig:f1 score in configurations with 16 refs (prec=1) dnn}} and having 0 as value in the detailed table under the figure. In this case, in many configurations can be seen an high false negative rate, but there are 2 critical performance degradation. 
The first one is an overfitting case in BEST Method, as fact GEOM\_MEDIAN outperforms it, underlying that at his threshold these distilled models recognize more outliers. It may be good in sort of a way, but at the same time the false negative rates are pretty high and this leads in making a choose between model optimization or choosing a higher number of reference d-vectors. 
Another necessity to make this choose are the models with good results, because the best performing one consists in 256-256 configuration achieving with BEST 45\%, with GEOM\_MEDIAN 54\% and with MEAN 61\%, which are the best ones, but it means the recall is even less (around 50\% in MEAN case) and the model does not fit on Syntiant. 
Increasing the references number to 64 references, are obtained the results in \textbf{Table \ref{fig:f1 score in configurations with 64 refs dnn}} and \textbf{Table \ref{fig:f1 score in configurations with 64 refs (prec=1) dnn}}. In EER minimization case, the results for not optimized or unbalanced versions have slightly worse results, instead in 256-256 which is the most consist one it should perform better, 
but the other are still good results which can be used in a general purpose use scenario. Instead, in no FP tolerance case, the problems of 16 references configurations are resolved, having as expected 256-256 that performs as the best one, having in BEST case over 80\% of F1 Score, indicating a Recall around 70\%, which can be acceptable, however the model does not fit on Syntiant, so if a user have to use a security approach on Syntiant should use 256-192 model, which in BEST case has a F1 score around 58\%, but a recall around 41\% which is not optimal in size terms and in performance, but using these unoptimized models these are the best results obtainable to obtain a precision of 1.
\begin{figure}[!h]
    \begin{minipage}[b]{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/5.07 F1 Score 64 DNN.png}
            \caption{F1 Score with 64 References ($<$EER)}
            \label{fig:f1 score in configurations with 64 refs dnn}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{images/5.08 F1 Score 64 DNN prec1.png}
            \caption{F1 Score with 64 References (PREC=1)}
            \label{fig:f1 score in configurations with 64 refs (prec=1) dnn}
    \end{minipage}
\end{figure}
\newpage
