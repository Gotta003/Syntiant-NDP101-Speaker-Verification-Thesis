\chapter{KWS and SV Models}
\label{cha:training}
\section{KWS Model}
\label{sec:kws deployment}
Keyword Spotting (KWS) is a model specialized in audio classification whose objective is to detect the presence of a spoken word or phrase according to an input sample, in this case, the spectrogram extracted by the MFE block. This transformation from raw data to spectrogram provides a time-frequency representation that emphasizes relevant acoustic features. A KWS model is trained to capture a predefined keyword by learning its spectral patterns across a range of samples, which must be differentiated and of big size to ensure generalization across speakers and noise conditions.
In this thesis, the KWS model is destined for deployment on the Syntiant NDP101 hardware. Due to the hardware constraints and design recommendations given by Syntiant, a DNN architecture has to be employed with a maximum output of 64 classes\cite{hardware_ndp101}. This network has 3 hidden fully connected layers and one output layer, used along with a softmax activation, mapping the feature vector into a probability distribution over defined class labels. The neural network structure strikes a balance between efficiency and classification accuracy, making it suited for real-time applications on embedded devices.
The focus of this work is on system verification. It allows the creation of a simple model consisting of one word, basically creating a binary output, that word or not. The chosen keyword for training is Sheila, selected due to its availability within the Google Speech Commands dataset \cite{speechcommands}. This offers a good number of audio recordings designed for keyword classification research, including many samples from multiple different speakers with varying environmental conditions.
The recordings in the dataset align with the Syntiant NDP101's input specifications because they are sampled at 16kHz and have a one-second duration per each sample. In addition to Sheila’s samples, a good dataset should include a variety of background noise sounds, samples of words not present in the dataset, and others similar phonetically, which may look alike from the neural network perspective. To give more samples as possible to the dataset, some audio clips were manually recorded or sourced from a dataset curated by Edge Impulse \cite{edgeimpulse_dataset_499022}, which is a platform specializing in machine learning for model development and deploying on edge devices. It is important to note that in the dataset provided, there are some Sheila samples, so to avoid any error in splitting, a checking is suggested.
The final dataset used consists of 3403 sheila words (almost 56m 43s of audio recording). These samples were divided 80\% for training and 20\% for evaluation. This division ensures that meaningful representations are learned while it is still providing independent data for validation.
To facilitate deployment on NDP101, the model is developed and trained using the Edge Impulse Studio\cite{edgeimpulse_kws_example}, which provides a good and intuitive pipeline for building and optimizing models tailored to the embedded hardware. The processes done by this framework include automatic data preprocessing, training, fine-tuning, and a quantization model conversion to int4, which is compatible with the Syntiant device. This workflow reduces development time, ensuring at the same time that the resulting model fits in the memory.
During and after training, confusion matrices are generated for classification performance. These provide insights into which keywords were misclassified, highlighting potential areas of confusion and suggesting model refinement. They are useful in identifying failure cases, such as confusion between similar-sounding words and to avoid this inconvenience, these words were already added to the dataset as others. The DNN model structure of KWS is the one in \textbf{Figure \ref{fig:keyword spotting model}}.
\begin{center}
  \begin{figure}[!h]
  \centering
  \includegraphics[width=0.975\textwidth]{images/3.01 KWS Model.png}
  \caption{Keyword Spotting Model}
  \label{fig:keyword spotting model}
\end{figure}
\end{center}
\section{SV Model}
\label{sec:sv introduction}
After creating the KWS model, the objective is to create a text-dependent Speaker Verification model, which requires only one train. This is known as ASV (Adaptive Speaker Verification), which relies on comparing the results of the model (d-vectors) with reference samples stored in the system dataset, which have to be captured during the inference phase. Speaker verification is used to recognize the identity of a user in an on-device learning context. To approach this way, a large amount of data is required at first to train the model, performing a meticulous extraction of d-vectors to recognize patterns in user voice and possibly trying to minimize the required number of samples, maximizing the security recognition. A sample, because of text dependency, will correspond to a word said by one user and should compare only with its similarities.\newline
The deployment of the model requires three caveats:\newline
1. Adapt directly to the device, meaning that a new user should be able to enroll in the SV application by providing voice samples in real time through the target device.\newline
2. The algorithm must operate in a one-class manner and should be able to learn to distinguish between the enrolled user and the others, only using the data from the dataset collected during inference.\newline
3. To fit a TinyML device the considerations should be done in memory allocation depending on the device used, so the model should fit in Flash Memory.\newline
To obtain the desired d-vector we should use a convolutional neural network because we would like to reduce the dimensionality while obtaining significant features. This is achieved with some expensive filters that, while reducing width and height, will generate new channels corresponding to a new feature extracted. This is a way to synthesize the input spectrogram (40 width x 40 height x 1 channel) to a lower dimension.\newline
A valid alternative, in theory, could be i-vectors. It is a characteristic that represents the characteristics of a distributive pattern of frame-level features. The extraction is a reduction in the dimensionality of the GMM supervector\cite{dvector_extractor_TinySV}, allowing an extraction per sentence; instead the d-vector generates a one-hot speaker label on the output, and it is an averaged activation from the last hidden layer of CNN. The advantage of the d-vector is that there is no assumption on the feature's distribution; instead, the i-vector assumes by default a Gaussian distribution.
\subsection{D-Vector Extractor Creation}
\label{subsec:d-vector extractor creation}
The structure of the CNN follows the theoretical one introduced and as input is given the spectrogram is given in output from the MFE block (40x40x1) and as output, the objective is a 256-size long array of relevant features (d-vector). However, it cannot be the model's output because, at first, the model should classify the data given in the input through a fully connected layer. As an input dataset, it is huge with many different speakers, each providing various samples because they come from audiobook recordings. Speakers' classification does not require the text-dependent approach, so the speakers will say many different words, which helps generate the required weights and biases for the convolution layers. The dataset used was from libreSpeech which has data collected per speaker and for this purpose the one with 100 hours of clean speech in the English language sampled with 16kHz was taken, which is the same as Syntiant audio processing\cite{librispeech}, corresponding to ~6GB memory space to make the training successful in reasonable times.\newline
The first problem is that these samples are not already in 1 second, but are variable, so they have been sampled and parsed through the MFE Block previously introduced. The total spectrograms obtained were 136112 for a total number of classes of 94, almost 38 hours of recording, and a memory occupation of 871MB. To have a balanced dataset, all classes had the same sample number, 1448. At first, they should have been 100, but they had too few samples. A convolution neural should perform three actions during its creation in which the dataset should subdivide its samples:\newline\newline
1. Training - The spectrogram is treated as an image, this dataset is the model's base, and on these the weights are adjusted using backpropagation, like Adam optimizer, and using loss functions like cross-entropy, for classification, or MSE, for regression, to minimize the failure rate. It uses a learning rate, which is a hyperparameter, to determine the step size at each iteration while moving forward with epochs. An epoch consists of a cycle of training input processing and an evaluation and can be arbitrarily set according to the complexity of the network. In this case, it was set to 700.
2. Validation - This is used to see the result on data other than those in the training set. It is used to adjust the learning rate and batch size in case it starts overfitting or to perform an early stopping if validation loss increases.\newline
3. Testing - It is a dataset to which background noise is added, varying microphone quality, and other modifications to the original input. It is like a validation but to stress out the system and see if it can still work with some fluctuations.\newline\newline
The distribution among these 3 sets is random inside a class, but each one will have an amount in each set. For precision, 70\% of the samples will go in training (95278), 15\% in validation (20417), and 15\% in testing (20417). In training with CNN, a batch size is suggested to avoid fluctuations, and considering that the samples may not be complete words, it performs an attenuation on those cases and the objective is having a good identification of the speaker. Knowing that a larger batch size will provide more accuracy, 32 sizes were chosen, which should be stable with standard learning functions. The setup of the model, as a summary, is an input shape of (32,40,40,1) [batch, width, height, channel] for 2977 inputs (95278/32) and an output shape of 94 classes. The model creation took about 3 hours using a GPU. The proposed architecture\cite{dvector_extractor_TinySV} corresponds to the one present in \textbf{Figure \ref{fig:speaker verification dvector extractor}}:
\begin{center}
    \begin{figure}[!h]
        \centering
        \includegraphics[width=1.0\textwidth]{images/3.02 D-vector Extractor.png}
        \caption{Speaker Verification Neural Network and D-vector Extractor}
        \label{fig:speaker verification dvector extractor}
    \end{figure}
\end{center}
After obtaining the CNN, it was implemented the Fully Connected Layers to catalog and what is expected is even with 700 epochs and 32 batches, to have a good accuracy but a bad loss, because the objective is to find patterns between very different samples. However, before the classification, at the Fully Connected layers' level, there is a generalized array of 256-size that represents the concept of d-vector, a general array that is a compression of the initial spectrogram and with max-pooling leverage relevant features, preserving all its consistency. Knowing this, we could truncate the DNN part of CNN and set the flatten section as the output, and it is possible because the two logic are separated.\newline
The problem that will arise now is how can the accuracy be computed, because before it all depended on classification, but truncating that last part requires another method to capture that value and because this is an ASV model, it will not be trained again. If it had been for a fixed user, for a specific usage, it would have been reasonable, but in this case, it is not the solution. The notion that was previously introduced consisted of cosine similarity. It takes two vectors and gives a percentage output on how much they are similar. The model that has just been created can provide a relevant reference of the audio sample given in input, and if in a dataset a similar one is provided, even if the spectrogram is not identical with feature compression and filtering they will be very similar. There are some existing solutions on which the model evaluation was performed:\newline
1. Best-matching: During inference, the reference samples have to be recorded in real time. It is recommended, because of this, to accept more than one sample and typically increase the number of samples saved because it is probable to find similar user samples. The best-matching technique saves per word said per user a number of vectors equal to the recommended size. It is important to note that because the d-vector elements are floats, a single reference vector occupies 1KB.\newline
2. Mean Reference: Instead of saving all the reference samples, occupying N KB per user's word enrolled, to save space, the average sample has to be computed. Theoretically, with this method, it would not have an optimal cosine similarity; however, it is a good trade-off in space-saving, and considering the operation on a TinyML the minimizing of storage and memory allocation is important. Some examples of this may be the arithmetical average and the geometrical median.\newline
In the case of Syntiant NDP101, due to the very small space, it is highly recommended to use the mean reference technique to minimize power consumption.
\subsection{Knowledge Distillation Training}
\label{subsec:knowledge distillation training}
If the solution can be deployed in some TinyML, it is not optimized and not compatible with Syntiant NDP101, because it can only support dense fully connected layer (DNN), but the d-vector extractor is a CNN\cite{distillation_from_cnn_to_dnn}\cite{knowledge_distillation}. In machine learning, there is a process called Knowledge Distillation, which is shown in \textbf{Figure \ref{fig:distillation model}}. It adapts two different models using each one with a different architecture but with equal input and output data. To do this, there should be a teacher and a student model. The teacher is a CNN and the student is a DNN that tries to replicate the results of the teacher at feeding of input data. The orientation of the student will be the loss discrepancy, which is 1-cosine similarity. To evaluate the model, the cosine similarity technique can be used for both, but the mean square error is an alternative solution, too. The advantages of this kind of approach are not only for deployment on NDP101, but dense layers have an easier computation than convolutional layers, which require more power consumption. As a consequence, the model would be faster. However, it should require more parameters to obtain similar results, because the 3 hidden layers structure remains fixed. In the end, the model would be less precise and occupy more memory but will be faster and deployable on NDP101.
\begin{center}
\begin{figure}[!h]
        \centering
        \includegraphics[width=1.0\textwidth]{images/3.03 Distillation Technique.png}
        \caption{Distillation Model from CNN to DNN \cite{knowledge_distillation}}
        \label{fig:distillation model}
    \end{figure}
\end{center}
The number of neurons per layer has to be chosen by the user, but there are some caveats to consider:\newline
• Syntiant NDP101 supports at most 589.000 parameters, so the neuron choice must be taken into account\newline
• Downscaling and then upscaling between two layers is not recommended and there should be a reduction in results performances.\newline
• Neurons typically have powers of 2 to optimize space and allow easier weight computation, so in case it is too inefficient, it is recommended to take at least powers of 2 or multiples of 32.\newline
\subsection{Quantization of SV Model}
\label{sec:quantization}
\begin{wrapfigure}{r}{0.35\textwidth}
  \begin{center}
    \includegraphics[width=0.35\textwidth]{images/3.04 Quantization Flow.jpg}
  \end{center}
  \caption{Quantization with Tensorflow \cite{pqt_tensorflow}}
  \label{fig:quantization with tensorflow}
\end{wrapfigure}
Unfortunately, the problems do not end here. Syntiant NDP101 not only has a DNN, but requires a parameter quantization, too. The weights should be stored in 4-bit\cite{description_ndp101} integers, meanwhile, biases in 32-bit integers. Performing a quantization is the most straightforward way, and in the case of transposing it into int-8 it is immediate, thanks to the support provided by Edge Impulse with Post-Quantization Training (PQT)\cite{pqt_tensorflow}, which is the case considering that the distilled model is trained as it is shown in \textbf{Figure \ref{fig:quantization with tensorflow}}. However, int-4 quantization is not optimized and as of now consists of a fake quantization, such as storing in int-8 bits an int-4.\cite{wu2023understandingint4quantizationtransformer} But, it can be seen that PQT int4 weights have really poor performances. Instead, performing a Quantization-Aware-Training (QAT)\cite{qat_tensorflow} as of now can achieve decent results, always using fake quantization. The problem is that it is unknown how the Syntiant binary is built and composed, so even with a good packing-unpacking technique, it could be difficult to obtain something from the model. Syntiant NDP101 provides a tool with the SDK to convert a model to binary Syntiant compatible, but it is under NDA. A solution could be using Edge Impulse Framework\cite{edgeimpulse_firmware_syntiant}, but it supports only classification and regression models as of now and is not what the d-vector extractor is doing.\newline
However, in theory, to successfully quantize the model, data must perform brute-force quantization and then fine-tuning to adjust values passing database data. An advantage of this is that the model's size stays not only in adjusted weights but reduces the intermediate input and output layers, too, leading to having an output no longer of 1KB, but of 256 bytes. It will save even more space in memory, however, to be sure to convert the input spectrogram in int, the model can be fake-quantize to leave the input and the output as floats adding a quantize and dequantize layer, the ones inside will be int8 and then manually remove from dequantize layer. This will allow maintenance of floating spectrogram logic, but at the same time allowing inference on Syntiant, if the conversion tool was available, and saving memory due to the reduction of 75\% per sample. The structure of the quantized model is shown in \textbf{Figure \ref{fig:quantized model int8}}.
\begin{center}
\begin{figure}[!h]
        \centering
        \includegraphics[width=1.0\textwidth]{images/3.05 Quantized Model Int8.png}
        \caption{Quantized Model Int8}
        \label{fig:quantized model int8}
    \end{figure}
\end{center}
\newpage